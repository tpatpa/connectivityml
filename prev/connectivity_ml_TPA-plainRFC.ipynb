{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using functional connectivity from the HCP to predict individual characteristics\n",
    "\n",
    "Connectivity ML Group @ [Neurohackademy 2021](https://neurohackademy.org/)\n",
    "\n",
    "## But first, import!"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# our core libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neuropythy as ny\n",
    "import nibabel as nib\n",
    "import ipyvolume as ipv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,plot_confusion_matrix\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import SGDRegressor, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import shap  # package used to calculate Shap values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# You need to configure neuropythy so that it knows what your\n",
    "# HCP AWS S3 access key and secret are:\n",
    "ny.config['hcp_credentials'] = (key, secret)\n",
    "\n",
    "ny.config['hcp_auto_download'] = True\n",
    "ny.config['hcp_auto_path'] = '~/hcp_data'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next, load the data..\n",
    "\n",
    "To `netmaps_df` we load \"netmaps\" which are subject-specific “parcellated connectomes” – for each subject, a nodes x nodes network matrix. See more [here](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP1200-DenseConnectome+PTN+Appendix-July2017.pdf).\n",
    "\n",
    "To `behavioral_df` we load the data keys. See more [here](https://wiki.humanconnectome.org/display/PublicData/HCP-YA+Data+Dictionary-+Updated+for+the+1200+Subject+Release). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "N = 15 # number of ICAs - 15, 25, 50 ,100 ,200 , 300"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "netmaps_df = pd.read_csv('data/connectivityml/HCP_PTN1200/netmats/3T_HCP1200_MSMAll_d'+str(N)+'_ts2/netmats2.txt', delim_whitespace=True,header=None)\n",
    "print(\"Network-matrices data shape:\", netmaps_df.shape)\n",
    "netmaps_df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network-matrices data shape: (1003, 225)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   0        1       2        3       4       5        6        7        8    \\\n",
       "0    0 -6.43160  2.9077   5.6367 -1.9108 -2.9030 -0.78089   1.6558 -0.46461   \n",
       "1    0 -4.09320 -4.8082   4.2359  1.2897 -6.0519  0.11149  -6.8054 -2.10540   \n",
       "2    0 -0.15331 -4.4759  11.8270 -4.2061 -6.5727 -1.61310 -10.2110 -1.28100   \n",
       "3    0 -0.87383 -6.8278  10.1640 -1.8393 -9.1321 -2.86400  -7.2182  1.55410   \n",
       "4    0 -3.96500  2.5420  11.5050 -4.9034 -2.4797  3.18270  -4.2678  1.84570   \n",
       "\n",
       "      9    ...     215      216      217     218     219     220     221  \\\n",
       "0  5.3488  ...  3.4614  0.89082 -4.27550  5.1550 -6.4641 -5.1331  5.8573   \n",
       "1  2.7563  ...  6.3430  0.35134 -0.67089  7.6578 -3.8168 -3.7186  1.4174   \n",
       "2  2.0158  ...  2.4110  3.49980 -1.95180  2.4394 -9.2378 -3.5996  4.5837   \n",
       "3  2.2322  ... -3.2454  0.34570 -1.92850  6.4472 -6.9826 -1.2231  6.6318   \n",
       "4  5.9304  ...  2.2238  7.66490  1.33870  8.1098 -1.4201 -1.3990  8.1015   \n",
       "\n",
       "        222     223  224  \n",
       "0 -2.723100  6.1086    0  \n",
       "1 -2.052100  5.0104    0  \n",
       "2 -3.172500  4.0976    0  \n",
       "3 -0.667030  7.3691    0  \n",
       "4  0.014264  1.4994    0  \n",
       "\n",
       "[5 rows x 225 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-6.43160</td>\n",
       "      <td>2.9077</td>\n",
       "      <td>5.6367</td>\n",
       "      <td>-1.9108</td>\n",
       "      <td>-2.9030</td>\n",
       "      <td>-0.78089</td>\n",
       "      <td>1.6558</td>\n",
       "      <td>-0.46461</td>\n",
       "      <td>5.3488</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4614</td>\n",
       "      <td>0.89082</td>\n",
       "      <td>-4.27550</td>\n",
       "      <td>5.1550</td>\n",
       "      <td>-6.4641</td>\n",
       "      <td>-5.1331</td>\n",
       "      <td>5.8573</td>\n",
       "      <td>-2.723100</td>\n",
       "      <td>6.1086</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-4.09320</td>\n",
       "      <td>-4.8082</td>\n",
       "      <td>4.2359</td>\n",
       "      <td>1.2897</td>\n",
       "      <td>-6.0519</td>\n",
       "      <td>0.11149</td>\n",
       "      <td>-6.8054</td>\n",
       "      <td>-2.10540</td>\n",
       "      <td>2.7563</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3430</td>\n",
       "      <td>0.35134</td>\n",
       "      <td>-0.67089</td>\n",
       "      <td>7.6578</td>\n",
       "      <td>-3.8168</td>\n",
       "      <td>-3.7186</td>\n",
       "      <td>1.4174</td>\n",
       "      <td>-2.052100</td>\n",
       "      <td>5.0104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.15331</td>\n",
       "      <td>-4.4759</td>\n",
       "      <td>11.8270</td>\n",
       "      <td>-4.2061</td>\n",
       "      <td>-6.5727</td>\n",
       "      <td>-1.61310</td>\n",
       "      <td>-10.2110</td>\n",
       "      <td>-1.28100</td>\n",
       "      <td>2.0158</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4110</td>\n",
       "      <td>3.49980</td>\n",
       "      <td>-1.95180</td>\n",
       "      <td>2.4394</td>\n",
       "      <td>-9.2378</td>\n",
       "      <td>-3.5996</td>\n",
       "      <td>4.5837</td>\n",
       "      <td>-3.172500</td>\n",
       "      <td>4.0976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.87383</td>\n",
       "      <td>-6.8278</td>\n",
       "      <td>10.1640</td>\n",
       "      <td>-1.8393</td>\n",
       "      <td>-9.1321</td>\n",
       "      <td>-2.86400</td>\n",
       "      <td>-7.2182</td>\n",
       "      <td>1.55410</td>\n",
       "      <td>2.2322</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.2454</td>\n",
       "      <td>0.34570</td>\n",
       "      <td>-1.92850</td>\n",
       "      <td>6.4472</td>\n",
       "      <td>-6.9826</td>\n",
       "      <td>-1.2231</td>\n",
       "      <td>6.6318</td>\n",
       "      <td>-0.667030</td>\n",
       "      <td>7.3691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-3.96500</td>\n",
       "      <td>2.5420</td>\n",
       "      <td>11.5050</td>\n",
       "      <td>-4.9034</td>\n",
       "      <td>-2.4797</td>\n",
       "      <td>3.18270</td>\n",
       "      <td>-4.2678</td>\n",
       "      <td>1.84570</td>\n",
       "      <td>5.9304</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2238</td>\n",
       "      <td>7.66490</td>\n",
       "      <td>1.33870</td>\n",
       "      <td>8.1098</td>\n",
       "      <td>-1.4201</td>\n",
       "      <td>-1.3990</td>\n",
       "      <td>8.1015</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>1.4994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 225 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "behavioral_df = pd.read_csv('data/connectivityml/unrestricted_pkalra_7_26_2021_17_39_25.csv')\n",
    "print(\"Behaviora data shape:\", behavioral_df.shape)\n",
    "behavioral_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have netmaps for 1003 subjects so we will need to filter `behavioral_df` a little.\n",
    "\n",
    "To `subjectsID_df` we load the ordered list of all subjects with complete rfMRI data (recon 1 + recon2) included in this PTN release"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "subjectsID_df = pd.read_csv('data/connectivityml/HCP_PTN1200/subjectIDs.txt',header=None,names=[\"Subject\"])\n",
    "print(\"Subjects ID data shape:\", subjectsID_df.shape)\n",
    "subjectsID_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that this corresponds to the number of netmaps we have."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filter_behavioral_df = subjectsID_df.merge(behavioral_df, on='Subject', how='inner')\n",
    "\n",
    "print(\"Filtered behaviora data shape:\", filter_behavioral_df.shape)\n",
    "filter_behavioral_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-process features matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "netmapsX_df = pd.DataFrame(data = netmaps_df, columns = range(N*N))\n",
    "netmapsX_df = netmapsX_df.T.drop_duplicates(keep='first').T\n",
    "netmapsX_df = netmapsX_df.T.drop_duplicates(keep='last').T\n",
    "X = netmapsX_df\n",
    "print(\"Features matrix shape:\", X.shape)\n",
    "X.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-process predicted values\n",
    "\n",
    "Here we are going to foucs on the subject gender."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filter_behavioral_df['Gender_i']=np.zeros(shape=(subjectsID_df.shape))\n",
    "filter_behavioral_df.Gender_i = pd.factorize(filter_behavioral_df.Gender)[0] # Encode the object as an enumerated type or categorical variable.\n",
    "y_gender = filter_behavioral_df.Gender_i # Gender of Subject\n",
    "print(\"y_gender shape:\", y_gender.shape)\n",
    "\n",
    "filter_behavioral_df['Gender_i'].groupby(filter_behavioral_df['Gender']).unique().apply(pd.Series).rename(columns={0:'Labels'}).sort_values(by='Labels')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.histplot(y_gender, ax=ax)\n",
    "ax.set_title(\"Gender of Subject\")\n",
    "fig.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time for Random forests!\n",
    "\n",
    "Data and estimators exploration was done in a separate notebook..\n",
    "\n",
    "### [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "Here we will prdict the gender of subjects in our dataset from our network-matrices data has gave us the best results..."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y_gender, test_size=0.2,stratify=y_gender,random_state=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "rf.fit(xtrain, ytrain) # Build a forest of trees from the training set (X, y).\n",
    "\n",
    "score = rf.score(xtest, ytest) # Return the mean accuracy on the given test data and labels.\n",
    "print(\"Test set score: \", score) \n",
    "\n",
    "ypred = rf.predict(xtest) # Predict class for X."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View confusion matrix for test data and predictions\n",
    "plot_confusion_matrix(rf, xtest, ytest) # Plot Confusion Matrix.\n",
    "plt.savefig('output/confusion_matrix_'+str(N)+'.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cr = classification_report(ytest, ypred,output_dict=True) # Build a text report showing the main classification metrics.\n",
    "sns_plot = sns.heatmap(pd.DataFrame(cr).iloc[:-1, :].T, annot=True) # plot scikit-learn classification report\n",
    "sns_plot.figure.savefig(\"output/classification_report_\"+str(N)+\".png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualizing trees\n",
    "We can plot individual decision trees - "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def plot_graphviz_tree(tree):\n",
    "    \"\"\"\n",
    "    Helper function that takes a tree as input, calls sklearn's export_graphviz\n",
    "    function to generate an image of the tree using graphviz, and then\n",
    "    plots the result in-line.\n",
    "    \"\"\"\n",
    "    export_graphviz(tree, out_file='tree.dot', max_depth=3, filled=True,\n",
    "                    feature_names=X.columns, impurity=False, rounded=True,\n",
    "                    proportion=False, precision=2);\n",
    "\n",
    "    call(['dot', '-Tpng', 'tree.dot', '-o', 'output/tree_'+str(N)+'.png', '-Gdpi=600'])\n",
    "    display(Image(filename = 'output/tree_'+str(N)+'.png'));"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# First tree in the forest\n",
    "plot_graphviz_tree(rf.estimators_[0]);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Interpreting random forests\n",
    "\n",
    "##### Feature importances\n",
    "\n",
    "Unlike regression-based methods, random forests don't have linear coefficientsso we look at the feature importances, to tell us how each feature contributes to the overall prediction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False).head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### SHAP values\n",
    "To interpret our results we will look at the SHAP values of our model.\n",
    "See more [here](https://towardsdatascience.com/explain-any-models-with-the-shap-values-use-the-kernelexplainer-79de9464897a).\n",
    "\n",
    "SHAP feature importance is an alternative to standard feature importance based on magnitude of feature attributions. The feature importance is useful, but contains no information beyond the importances. For a more informative plot, we will next look at the summary plot.\n",
    "\n",
    "The SHAP summary plot is made of all the dots in the test data. Showing:\n",
    "- **Feature importance:** Variables are ranked in descending order.\n",
    "- **Impact:** The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n",
    "- **Original value:** Color shows whether that variable is high (in red) or low (in blue) for that observation.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of xtest rather than a single row, to have more data for plot.\n",
    "shap_values = explainer.shap_values(xtest)\n",
    "\n",
    "# Make plot. Index of [1] is explained in text below.\n",
    "\n",
    "fig = shap.summary_plot(shap_values[1], xtest, show=False)\n",
    "plt.savefig('output/summary_plot_xtest_'+str(N)+'.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Single subject interpretation\n",
    "We will look at SHAP values for a random row of the dataset.\n",
    "\n",
    "##### SHAP values\n",
    "For context, we'll look at the raw predictions before looking at the SHAP values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "row_to_show = 5\n",
    "data_for_prediction = xtest.loc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "rf.predict_proba(data_for_prediction_array)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Subject evaluated:\",subjectsID_df.Subject[row_to_show])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "shap.initjs()\n",
    "\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=False)\n",
    "# plt.savefig('force_plot_hcp_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The SHAP values of all features sum up to explain why the prediction was different from the baseline. This allows us to decompose a prediction in a graph like this where:\n",
    "\n",
    "- The **output value** is the prediction for that observation (the prediction of the subject evluated).\n",
    " - The **base value** is the value that would be predicted if we did not know any features for the current output (the mean prediction, or mean(yhat)).\n",
    " - **Red/blue:** Features that push the prediction higher (to the right) are shown in red, and those pushing the prediction lower are in blue.\n",
    " \n",
    "##### TOP 5 Most important ICA's"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vals= np.abs(shap_values).mean(0)\n",
    "feature_importance = pd.DataFrame(list(zip(xtest.columns,vals)),columns=['netmat_col','feature_importance_vals'])\n",
    "feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n",
    "feature_importance.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edge_df = pd.DataFrame(columns = ['netmat_col','Node1', 'Node2', 'Weight'])\n",
    "countlist = list() # if node1,node2 weight is saved no need to save node2,node1\n",
    "\n",
    "for index, row in tqdm(netmaps_df.iterrows(), total=netmaps_df.shape[0]):\n",
    "    netmat_col = 0\n",
    "    if index == row_to_show:\n",
    "        row2mat = row.values.reshape(N,N)\n",
    "        for node1 in range(N):\n",
    "            for node2 in range(N):\n",
    "                if node1!=node2:\n",
    "                    if (node2, node1) not in countlist:\n",
    "                        countlist.append((node1, node2))\n",
    "                        curr_edge = {'netmat_col': netmat_col, 'Node1': node1, 'Node2': node2, 'Weight':row2mat[node1][node2]}\n",
    "                        edge_df = edge_df.append(curr_edge, ignore_index = True)\n",
    "                netmat_col = netmat_col + 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = open('output/top5_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.txt', \"w\")\n",
    "\n",
    "for i in range(5):\n",
    "    feature = feature_importance.netmat_col.values[i]\n",
    "    print(\"Important feature #: \",feature)\n",
    "    f.write(\"Important feature #: \" + str(feature)+\"\\n\")\n",
    "    ICA_Node1 = edge_df.loc[edge_df['netmat_col'] == feature, 'Node1']\n",
    "    ICA_Node2 = edge_df.loc[edge_df['netmat_col'] == feature, 'Node2']\n",
    "\n",
    "    print(\"Node 1:\",ICA_Node1.iloc[0])\n",
    "    f.write(\"Node 1: \" + str(ICA_Node1.iloc[0])+\"\\n\")\n",
    "    print(\"Node 2:\",ICA_Node2.iloc[0])\n",
    "    f.write(\"Node 2: \" + str(ICA_Node2.iloc[0])+\"\\n\")\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Knowing the ICA's that are part of the most important feature we can plot them on our single subject brain"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get a sample HCP subject:\n",
    "sub = ny.hcp_subject(subjectsID_df.Subject[row_to_show])\n",
    "sub"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the CIFTI file:\n",
    "cii_filename = '~/data/connectivityml/HCP_PTN1200/groupICA/groupICA_3T_HCP1200_MSMAll_d'+str(N)+'.ica/melodic_IC.dscalar.nii'\n",
    "cii_obj = ny.load(cii_filename)\n",
    "\n",
    "# Split the CIFTI object into hemisphere/subvoxel data:\n",
    "(lh_data, rh_data, subvox_data) = ny.hcp.cifti_split(cii_obj)\n",
    "\n",
    "# These data should be (N(data-points) x vertices)\n",
    "lh_data.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sub.lh and sub.rh are the \"native\" (FreeSurfer) hemispheres;\n",
    "# sub.hemis['lh_LR32k'] and sub.hemis['rh_LR32k'] are the\n",
    "# HCP subject-aligned fs_LR hemispheres (with 32k resolution).\n",
    "lh_hemi_native = sub.lh\n",
    "rh_hemi_native = sub.rh\n",
    "\n",
    "# The 32492 size indicates this is a 32k LR hemisphere:\n",
    "lh_hemi = sub.hemis['lh_LR32k']\n",
    "rh_hemi = sub.hemis['rh_LR32k']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ICA 1**"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We can make an ipyvolume figure to plot both hemispheres on:\n",
    "fig = ipv.figure()\n",
    "# Then plot each hemisphere using whichever ICA component we\n",
    "# want to visualize:\n",
    "\n",
    "ICA_Node1 = int(edge_df.loc[edge_df['netmat_col'] == feature_importance.netmat_col.values[0], 'Node1'])\n",
    "ny.cortex_plot(lh_hemi, surface='inflated', color=lh_data[ICA_Node1], cmap='hot', figure=fig)\n",
    "ny.cortex_plot(rh_hemi, surface='inflated', color=rh_data[ICA_Node1], cmap='hot', figure=fig)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ipv.pylab.save('output/most_important_ICA1_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.html',title='Most important feature ICA 1')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ICA 2**"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We can make an ipyvolume figure to plot both hemispheres on:\n",
    "fig = ipv.figure()\n",
    "# Then plot each hemisphere using whichever ICA component we\n",
    "# want to visualize:\n",
    "\n",
    "ICA_Node2 = int(edge_df.loc[edge_df['netmat_col'] == feature_importance.netmat_col.values[0], 'Node2'])\n",
    "ny.cortex_plot(lh_hemi, surface='inflated', color=lh_data[ICA_Node2], cmap='hot', figure=fig)\n",
    "ny.cortex_plot(rh_hemi, surface='inflated', color=rh_data[ICA_Node2], cmap='hot', figure=fig)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ipv.pylab.save('output/most_important_ICA2_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.html',title='Most important feature ICA 2')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}